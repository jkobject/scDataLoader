{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ found cached instance metadata: /home/ml4ig1/.lamin/instance--jkobject--scdataloader.env\n",
      "ðŸ’¡ loaded instance: jkobject/scdataloader\n",
      "ðŸ’¡ loaded instance: jkobject/scdataloader\n"
     ]
    }
   ],
   "source": [
    "! lamin load scdataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ lamindb instance: jkobject/scdataloader\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "from scdataloader import DataModule\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = DataModule(\n",
    "    collection_name=\"preprocessed dataset\",\n",
    "    organisms=[\"NCBITaxon:9606\"], #organism that we will work on\n",
    "    how=\"most expr\", # for the collator (most expr genes only will be selected)\n",
    "    max_len=1000, # only the 1000 most expressed\n",
    "    batch_size=64,\n",
    "    num_workers=1,\n",
    "    validation_split=0.1,\n",
    "    test_split=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## or can be a much more complex dataloader too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_labels = [\n",
    "    \"cell_type_ontology_term_id\",\n",
    "    #\"tissue_ontology_term_id\"\n",
    "    \"disease_ontology_term_id\",\n",
    "    #\"development_stage_ontology_term_id\",\n",
    "    \"assay_ontology_term_id\",\n",
    "    'self_reported_ethnicity_ontology_term_id',\n",
    "]\n",
    "labels_to_pred = hierarchical_labels+[\n",
    "    'sex_ontology_term_id',\n",
    "    \"organism_ontology_term_id\",\n",
    "]\n",
    "all_labels = labels_to_pred+[\n",
    "    #'dataset_id',\n",
    "    #'cell_culture',\n",
    "    \"heat_diff\",\n",
    "    \"total_counts\",\n",
    "    \"nnz\",\n",
    "    \"dpt_group\",\n",
    "]\n",
    "\n",
    "name=\"preprocessed dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loader\n",
    "\n",
    "to create the dataloader we need a lamindb dataset. Here we take the one that we created in the previous notebook, but it can be another dataset like the lamin's cellxgene dataset.\n",
    "\n",
    "example:\n",
    "```python\n",
    "dataset = ln.Collection.using(\"laminlabs/cellxgene\").one()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "won't do any check but we recommend to have your dataset coming from local storage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% are aligned\n",
      "total dataset size is 0.917606818 Gb\n",
      "---\n",
      "dataset contains:\n",
      "     23349 cells\n",
      "     70116 genes\n",
      "     10 labels\n",
      "     1 organisms\n",
      "dataset contains 40 classes to predict\n",
      "\n",
      "downloading gene names from biomart\n",
      "['ensembl_gene_id', 'hgnc_symbol', 'gene_biotype', 'entrezgene_id', 'start_position', 'chromosome_name']\n",
      "['ensembl_gene_id', 'hgnc_symbol', 'gene_biotype', 'entrezgene_id', 'start_position', 'chromosome_name']\n",
      "reduced the size to 0.6722574020195106\n"
     ]
    }
   ],
   "source": [
    "datamodule = DataModule(\n",
    "    collection_name=\"preprocessed dataset\",\n",
    "    all_labels=all_labels, #all the labels to query in the obs field\n",
    "    hierarchical_labels=hierarchical_labels, #labels that can benefit from ontological hierarchies \n",
    "    organisms=[\"NCBITaxon:9606\"], #organism that we will work on\n",
    "    how=\"most expr\", # for the collator (most expr genes only will be selected)\n",
    "    max_len=1000, # only the 1000 most expressed\n",
    "    add_zero_genes=100, #some additional zeros will be given\n",
    "    label_to_weight=labels_to_pred, # for weighted random sampling\n",
    "    label_to_pred=labels_to_pred,\n",
    "    batch_size=64,\n",
    "    num_workers=1,\n",
    "    validation_split=0.2,\n",
    "    test_split=0)\n",
    "\n",
    "# we setup the datamodule (as exemplified in lightning's good practices, but there might be some things to improve here)\n",
    "testfiles = datamodule.setup() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/292 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[ 78.,   6.,   6.,  ...,   0.,   0.,   0.],\n",
      "        [141.,  75.,  58.,  ...,   0.,   0.,   0.],\n",
      "        [309.,  50.,  31.,  ...,   0.,   0.,   0.],\n",
      "        ...,\n",
      "        [157., 108.,  79.,  ...,   0.,   0.,   1.],\n",
      "        [303., 123.,  70.,  ...,   0.,   0.,   0.],\n",
      "        [136.,  29.,  22.,  ...,   0.,   0.,   0.]]), 'genes': tensor([[41514,   725,  9560,  ..., 23989, 20098, 39181],\n",
      "        [41514, 15694,  9164,  ..., 47038, 10040, 54239],\n",
      "        [41514, 16072, 12461,  ..., 59205, 16411, 67531],\n",
      "        ...,\n",
      "        [41514,  1583,  8960,  ..., 62974, 57751, 14310],\n",
      "        [41514, 13107,  9164,  ..., 20352, 32101,  9779],\n",
      "        [41514, 15694,   409,  ..., 50807, 36053, 38710]], dtype=torch.int32), 'class': tensor([[ 2,  0,  0,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0],\n",
      "        [12,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0],\n",
      "        [ 9,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0],\n",
      "        [ 9,  0,  0,  0,  0,  0],\n",
      "        [11,  0,  0,  0,  0,  0],\n",
      "        [11,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0],\n",
      "        [12,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0],\n",
      "        [10,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0],\n",
      "        [ 9,  0,  0,  0,  0,  0],\n",
      "        [11,  0,  0,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0,  0],\n",
      "        [11,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0],\n",
      "        [ 9,  0,  0,  0,  0,  0],\n",
      "        [11,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0],\n",
      "        [11,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0,  0],\n",
      "        [11,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0],\n",
      "        [11,  0,  0,  0,  0,  0],\n",
      "        [ 8,  0,  0,  0,  0,  0],\n",
      "        [ 8,  0,  0,  0,  0,  0],\n",
      "        [ 8,  0,  0,  0,  0,  0],\n",
      "        [10,  0,  0,  0,  0,  0],\n",
      "        [ 9,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0],\n",
      "        [12,  0,  0,  0,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0,  0],\n",
      "        [ 8,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0],\n",
      "        [ 8,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0],\n",
      "        [ 9,  0,  0,  0,  0,  0],\n",
      "        [11,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0],\n",
      "        [ 9,  0,  0,  0,  0,  0],\n",
      "        [10,  0,  0,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0]], dtype=torch.int32), 'tp': tensor([1.2580e-03, 4.4219e-02, 3.0407e-04, 1.3363e-02, 5.3897e-05, 7.4358e-02,\n",
      "        1.6140e-04, 2.5554e-02, 5.0249e-02, 1.0121e-02, 1.9645e-02, 3.8997e-02,\n",
      "        2.7244e-04, 3.5064e-04, 1.6611e-03, 2.9493e-04, 6.1022e-03, 2.9618e-04,\n",
      "        2.9830e-04, 1.0688e-02, 9.9674e-02, 8.6086e-02, 1.4521e-02, 4.0110e-02,\n",
      "        1.9823e-02, 9.0700e-03, 3.5943e-02, 4.2530e-03, 4.3240e-02, 2.6298e-03,\n",
      "        3.0275e-04, 2.4445e-02, 7.9859e-03, 2.3292e-04, 2.0356e-02, 1.8703e-02,\n",
      "        3.1378e-04, 6.5560e-02, 1.5749e-01, 9.5593e-02, 1.0728e-01, 1.3018e-02,\n",
      "        3.5483e-02, 1.1571e-02, 3.3617e-02, 1.3363e-02, 3.1799e-02, 3.3795e-02,\n",
      "        1.1277e-01, 2.0618e-04, 1.4773e-04, 2.7142e-04, 1.7224e-01, 1.7291e-04,\n",
      "        1.5910e-04, 4.7466e-03, 1.1477e-04, 7.6637e-02, 6.4210e-02, 3.8356e-03,\n",
      "        9.0700e-03, 1.3018e-02, 3.1949e-02, 2.9733e-04]), 'depth': tensor([ 1149.,  6478.,  4444.,  3980.,  6850.,  2841.,  5844.,  2151.,  5164.,\n",
      "         5571.,  3609.,  3607.,  4621.,  3708.,  4482.,  2663., 23807.,  3917.,\n",
      "         4050.,  2161.,  6427.,  2605.,  2266.,  2034.,  9118.,  2563.,  1504.,\n",
      "         1601.,  2837.,  1645.,  4130.,  8535., 15514.,  2105.,  3051.,  1500.,\n",
      "         3049.,  2328.,  8889.,  2762.,  5223.,  6030.,  1138.,  1702.,  4462.,\n",
      "         3980.,  7410.,  7727.,  4714.,  2563.,  1984.,  5015.,  9833.,  1977.,\n",
      "        10714.,  1554.,  3896.,  6008.,  2098.,  1203.,  2563.,  6030.,  7775.,\n",
      "         2645.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/292 [00:03<?, ?it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(datamodule.train_dataloader()):\n",
    "    # pass #or do pass\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. \n",
    "# with lightning:\n",
    "# Trainer(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (WIP) build a set of different collators that can be used to preprocess the minibatches before feeding them to the model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scprint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
