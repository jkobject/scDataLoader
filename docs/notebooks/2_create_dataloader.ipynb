{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scdataloader import Dataset\n",
        "from scdataloader import DataLoader\n",
        "import pandas as pd\n",
        "import lamindb as ln\n",
        "import lnschema_bionty as lb\n",
        "\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## var definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "organism = lb.Organism.filter(ontology_id=\"NCBITaxon:9606\").one()\n",
        "genedf = lb.Gene.filter(organism_id=organism.id).df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Var location \n",
        "\n",
        "here we decide to add another layer of information where we provide a gene's rough location estimation in the dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading gene names from biomart\n",
            "\n",
            "['ensembl_gene_id', 'hgnc_symbol', 'gene_biotype', 'entrezgene_id', 'start_position', 'chromosome_name']\n"
          ]
        }
      ],
      "source": [
        "from scdataloader.utils import getBiomartTable\n",
        "\n",
        "biomart = getBiomartTable(attributes=['start_position', 'chromosome_name']).set_index('ensembl_gene_id')\n",
        "genedf = genedf.set_index('ensembl_gene_id')\n",
        "\n",
        "genedf = genedf.loc[~genedf.index.duplicated(keep='first')]\n",
        "biomart = biomart.loc[~biomart.index.duplicated(keep='first')]\n",
        "\n",
        "genedf = genedf.join(biomart[['start_position', 'chromosome_name']], how='inner').sort_values(by=['chromosome_name', 'start_position'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reduced the size to 0.6722574020195106\n",
            "\n"
          ]
        }
      ],
      "source": [
        "c = []\n",
        "i = 0\n",
        "prev_position = -100000\n",
        "prev_chromosome = None\n",
        "for _, r in genedf.iterrows():\n",
        "    if r['chromosome_name'] != prev_chromosome or r['start_position'] - prev_position > 10_000:\n",
        "        i += 1\n",
        "    c.append(i)\n",
        "    prev_position = r['start_position']\n",
        "    prev_chromosome = r['chromosome_name']\n",
        "print(f'reduced the size to {len(set(c))/len(genedf)}')\n",
        "genedf['group'] = c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#[File(uid='AnalH1SNJ2cQ7SVtsAvg', suffix='.h5ad', accessor='AnnData', description='preprocessed by scprint', version='2', size=59079604, hash='4f0no-pjg35qG--75wu5JZ', hash_type='sha1-fl', visibility=1, key_is_virtual=True, updated_at=2023-12-12 13:16:03 UTC, storage_id=1, initial_version_id=990, created_by_id=1), \n",
        "\n",
        "#File(uid='qsmZFgVcwPqVN9h23x6p', suffix='.h5ad', accessor='AnnData', description='preprocessed by scprint', version='2', size=82350434, hash='lUJl8wVAqHv1WM829YtELW', hash_type='sha1-fl', visibility=1, key_is_virtual=True, updated_at=2023-12-12 13:27:33 UTC, storage_id=1, initial_version_id=1034, created_by_id=1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### optional: var embeddings\n",
        "\n",
        "Many novel models like transformers work on embeddings of the variable field. This can be learnt or provided like it is done here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# see scprint for this or contact me (@jkobject)\n",
        "embeddings = embed(genedf=genedf,\n",
        "    organism=\"homo_sapiens\",\n",
        "    cache=True,\n",
        "    fasta_path=\"/tmp/data/fasta/\",\n",
        "    embedding_size=1024,)\n",
        "embeddings.to_parquet('../../data/temp/embeddings.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings = pd.read_parquet('../../data/temp/embeddings.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## data loader\n",
        "\n",
        "to create the dataloader we need a lamindb dataset. Here we take the one that we created in the previous notebook, but it can be another dataset like the lamin's cellxgene dataset.\n",
        "\n",
        "example:\n",
        "```python\n",
        "dataset = ln.Collection.using(\"laminlabs/cellxgene\").one()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# OR directly load the dataset\n",
        "name=\"preprocessed dataset\"\n",
        "dataset = ln.Collection.filter(name=name).one()\n",
        "dataset.artifacts.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the dataloader can weight some rare samples more: \n",
        "# one need to provide the labels on which to weight the samples:\n",
        "labels_weighted_sampling = [\n",
        "    'sex_ontology_term_id',\n",
        "    \"cell_type_ontology_term_id\",\n",
        "    #\"tissue_ontology_term_id\",\n",
        "    \"disease_ontology_term_id\",\n",
        "    #\"development_stage_ontology_term_id\",\n",
        "    \"assay_ontology_term_id\",\n",
        "    'self_reported_ethnicity_ontology_term_id',\n",
        "]\n",
        "\n",
        "# the dataloader can also output some obs field\n",
        "all_labels = labels_weighted_sampling+[\n",
        "    #'dataset_id',\n",
        "    #'cell_culture',\n",
        "    \"dpt_group\",\n",
        "    \"heat_diff\",\n",
        "    \"nnz\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❗ no run & transform get linked, consider passing a `run` or calling ln.track()\n",
            "won't do any check but we recommend to have your dataset coming from local storage\n",
            "❗ no run & transform get linked, consider passing a `run` or calling ln.track()\n",
            "total dataset size is 23.47712381 Gb\n",
            "---\n",
            "❗ no run & transform get linked, consider passing a `run` or calling ln.track()\n",
            "total dataset size is 23.47712381 Gb\n",
            "---\n",
            "dataset contains:\n",
            "     1582328 cells\n",
            "     70116 genes\n",
            "     8 labels\n",
            "     1 organisms\n",
            "dataset contains 113 classes to predict\n",
            "embedding size is 1024\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#we then create a mapped dataset. This transforms a bunch of anndata from possibly various species, into a combined object that acts roughly as a single anndata dataset \n",
        "# (WIP to get all the features of an anndata object) \n",
        "mdataset = Dataset(dataset, genedf, gene_embedding=embeddings, organisms=[\"NCBITaxon:9606\"], obs=all_labels, encode_obs=labels_weighted_sampling)\n",
        "mdataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# now we make the dataloader\n",
        "dataloader = DataLoader(mdataset, label_to_weight=labels_weighted_sampling, batch_size=4, num_workers=1)\n",
        "len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64), tensor([30, 65, 78,  4]), tensor([5, 3, 3, 5]), tensor([1, 1, 1, 1]), tensor([2, 5, 3, 3]), tensor([0, 0, 1, 1]), ('13_MONDO:0100320_CL:0001062_UBERON:0000178', '2_PATO:0000461_CL:0000907_UBERON:0000178', '0_PATO:0000461_CL:0000938_UBERON:0000178', '7_MONDO:0100320_CL:0000794_UBERON:0000178'), tensor([0.0027, 0.0066, 0.0040, 0.0029], dtype=torch.float64), tensor([1206, 1953, 1005,  787])]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in dataloader:\n",
        "    print(i)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (WIP) build a set of different collators that can be used to preprocess the minibatches before feeding them to the model "
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
