{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ found cached instance metadata: /home/ml4ig1/.lamin/instance--jkobject--scdataloader.env\n",
            "ðŸ’¡ loaded instance: jkobject/scdataloader\n",
            "ðŸ’¡ loaded instance: jkobject/scdataloader\n"
          ]
        }
      ],
      "source": [
        "! lamin load scdataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ lamindb instance: jkobject/scdataloader\n"
          ]
        }
      ],
      "source": [
        "from scdataloader import DataModule\n",
        "import tqdm\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datamodule = DataModule(\n",
        "    collection_name=\"preprocessed dataset\",\n",
        "    organisms=[\"NCBITaxon:9606\"], #organism that we will work on\n",
        "    how=\"most expr\", # for the collator (most expr genes only will be selected)\n",
        "    max_len=1000, # only the 1000 most expressed\n",
        "    batch_size=64,\n",
        "    num_workers=1,\n",
        "    validation_split=0.1,\n",
        "    test_split=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## or can be a much more complex dataloader too!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "hierarchical_labels = [\n",
        "    \"cell_type_ontology_term_id\",\n",
        "    #\"tissue_ontology_term_id\"\n",
        "    \"disease_ontology_term_id\",\n",
        "    #\"development_stage_ontology_term_id\",\n",
        "    \"assay_ontology_term_id\",\n",
        "    'self_reported_ethnicity_ontology_term_id',\n",
        "]\n",
        "labels_to_pred = hierarchical_labels+[\n",
        "    'sex_ontology_term_id',\n",
        "    \"organism_ontology_term_id\",\n",
        "]\n",
        "all_labels = labels_to_pred+[\n",
        "    #'dataset_id',\n",
        "    #'cell_culture',\n",
        "    \"heat_diff\",\n",
        "    \"total_counts\",\n",
        "    \"nnz\",\n",
        "    \"dpt_group\",\n",
        "]\n",
        "\n",
        "name=\"preprocessed dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## data loader\n",
        "\n",
        "to create the dataloader we need a lamindb dataset. Here we take the one that we created in the previous notebook, but it can be another dataset like the lamin's cellxgene dataset.\n",
        "\n",
        "example:\n",
        "```python\n",
        "dataset = ln.Collection.using(\"laminlabs/cellxgene\").one()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "won't do any check but we recommend to have your dataset coming from local storage\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100.0% are aligned\n",
            "total dataset size is 0.917606818 Gb\n",
            "---\n",
            "dataset contains:\n",
            "     23349 cells\n",
            "     70116 genes\n",
            "     10 labels\n",
            "     1 organisms\n",
            "dataset contains 40 classes to predict\n",
            "\n",
            "downloading gene names from biomart\n",
            "['ensembl_gene_id', 'hgnc_symbol', 'gene_biotype', 'entrezgene_id', 'start_position', 'chromosome_name']\n",
            "['ensembl_gene_id', 'hgnc_symbol', 'gene_biotype', 'entrezgene_id', 'start_position', 'chromosome_name']\n",
            "reduced the size to 0.6722574020195106\n"
          ]
        }
      ],
      "source": [
        "datamodule = DataModule(\n",
        "    collection_name=\"preprocessed dataset\",\n",
        "    all_labels=all_labels, #all the labels to query in the obs field\n",
        "    hierarchical_labels=hierarchical_labels, #labels that can benefit from ontological hierarchies \n",
        "    organisms=[\"NCBITaxon:9606\"], #organism that we will work on\n",
        "    how=\"most expr\", # for the collator (most expr genes only will be selected)\n",
        "    max_len=1000, # only the 1000 most expressed\n",
        "    add_zero_genes=100, #some additional zeros will be given\n",
        "    label_to_weight=labels_to_pred, # for weighted random sampling\n",
        "    label_to_pred=labels_to_pred,\n",
        "    batch_size=64,\n",
        "    num_workers=1,\n",
        "    validation_split=0.2,\n",
        "    test_split=0)\n",
        "\n",
        "# we setup the datamodule (as exemplified in lightning's good practices, but there might be some things to improve here)\n",
        "testfiles = datamodule.setup() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/292 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'x': tensor([[ 78.,   6.,   6.,  ...,   0.,   0.,   0.],\n",
            "        [141.,  75.,  58.,  ...,   0.,   0.,   0.],\n",
            "        [309.,  50.,  31.,  ...,   0.,   0.,   0.],\n",
            "        ...,\n",
            "        [157., 108.,  79.,  ...,   0.,   0.,   1.],\n",
            "        [303., 123.,  70.,  ...,   0.,   0.,   0.],\n",
            "        [136.,  29.,  22.,  ...,   0.,   0.,   0.]]), 'genes': tensor([[41514,   725,  9560,  ..., 23989, 20098, 39181],\n",
            "        [41514, 15694,  9164,  ..., 47038, 10040, 54239],\n",
            "        [41514, 16072, 12461,  ..., 59205, 16411, 67531],\n",
            "        ...,\n",
            "        [41514,  1583,  8960,  ..., 62974, 57751, 14310],\n",
            "        [41514, 13107,  9164,  ..., 20352, 32101,  9779],\n",
            "        [41514, 15694,   409,  ..., 50807, 36053, 38710]], dtype=torch.int32), 'class': tensor([[ 2,  0,  0,  0,  0,  0],\n",
            "        [ 7,  0,  0,  0,  0,  0],\n",
            "        [ 3,  0,  0,  0,  0,  0],\n",
            "        [12,  0,  0,  0,  0,  0],\n",
            "        [ 4,  0,  0,  0,  0,  0],\n",
            "        [ 9,  0,  0,  0,  0,  0],\n",
            "        [ 4,  0,  0,  0,  0,  0],\n",
            "        [ 7,  0,  0,  0,  0,  0],\n",
            "        [ 0,  0,  0,  0,  0,  0],\n",
            "        [ 9,  0,  0,  0,  0,  0],\n",
            "        [11,  0,  0,  0,  0,  0],\n",
            "        [11,  0,  0,  0,  0,  0],\n",
            "        [ 3,  0,  0,  0,  0,  0],\n",
            "        [ 3,  0,  0,  0,  0,  0],\n",
            "        [12,  0,  0,  0,  0,  0],\n",
            "        [ 4,  0,  0,  0,  0,  0],\n",
            "        [10,  0,  0,  0,  0,  0],\n",
            "        [ 3,  0,  0,  0,  0,  0],\n",
            "        [ 3,  0,  0,  0,  0,  0],\n",
            "        [ 2,  0,  0,  0,  0,  0],\n",
            "        [ 9,  0,  0,  0,  0,  0],\n",
            "        [11,  0,  0,  0,  0,  0],\n",
            "        [ 7,  0,  0,  0,  0,  0],\n",
            "        [11,  0,  0,  0,  0,  0],\n",
            "        [ 3,  0,  0,  0,  0,  0],\n",
            "        [ 9,  0,  0,  0,  0,  0],\n",
            "        [11,  0,  0,  0,  0,  0],\n",
            "        [ 2,  0,  0,  0,  0,  0],\n",
            "        [11,  0,  0,  0,  0,  0],\n",
            "        [ 2,  0,  0,  0,  0,  0],\n",
            "        [ 4,  0,  0,  0,  0,  0],\n",
            "        [ 7,  0,  0,  0,  0,  0],\n",
            "        [ 3,  0,  0,  0,  0,  0],\n",
            "        [ 3,  0,  0,  0,  0,  0],\n",
            "        [ 7,  0,  0,  0,  0,  0],\n",
            "        [11,  0,  0,  0,  0,  0],\n",
            "        [ 3,  0,  0,  0,  0,  0],\n",
            "        [11,  0,  0,  0,  0,  0],\n",
            "        [ 8,  0,  0,  0,  0,  0],\n",
            "        [ 8,  0,  0,  0,  0,  0],\n",
            "        [ 8,  0,  0,  0,  0,  0],\n",
            "        [10,  0,  0,  0,  0,  0],\n",
            "        [ 9,  0,  0,  0,  0,  0],\n",
            "        [ 2,  0,  0,  0,  0,  0],\n",
            "        [ 0,  0,  0,  0,  0,  0],\n",
            "        [12,  0,  0,  0,  0,  0],\n",
            "        [ 1,  0,  0,  0,  0,  0],\n",
            "        [ 7,  0,  0,  0,  0,  0],\n",
            "        [ 8,  0,  0,  0,  0,  0],\n",
            "        [ 4,  0,  0,  0,  0,  0],\n",
            "        [ 3,  0,  0,  0,  0,  0],\n",
            "        [ 4,  0,  0,  0,  0,  0],\n",
            "        [ 8,  0,  0,  0,  0,  0],\n",
            "        [ 3,  0,  0,  0,  0,  0],\n",
            "        [ 4,  0,  0,  0,  0,  0],\n",
            "        [ 0,  0,  0,  0,  0,  0],\n",
            "        [ 3,  0,  0,  0,  0,  0],\n",
            "        [ 9,  0,  0,  0,  0,  0],\n",
            "        [11,  0,  0,  0,  0,  0],\n",
            "        [ 2,  0,  0,  0,  0,  0],\n",
            "        [ 9,  0,  0,  0,  0,  0],\n",
            "        [10,  0,  0,  0,  0,  0],\n",
            "        [ 7,  0,  0,  0,  0,  0],\n",
            "        [ 4,  0,  0,  0,  0,  0]], dtype=torch.int32), 'tp': tensor([1.2580e-03, 4.4219e-02, 3.0407e-04, 1.3363e-02, 5.3897e-05, 7.4358e-02,\n",
            "        1.6140e-04, 2.5554e-02, 5.0249e-02, 1.0121e-02, 1.9645e-02, 3.8997e-02,\n",
            "        2.7244e-04, 3.5064e-04, 1.6611e-03, 2.9493e-04, 6.1022e-03, 2.9618e-04,\n",
            "        2.9830e-04, 1.0688e-02, 9.9674e-02, 8.6086e-02, 1.4521e-02, 4.0110e-02,\n",
            "        1.9823e-02, 9.0700e-03, 3.5943e-02, 4.2530e-03, 4.3240e-02, 2.6298e-03,\n",
            "        3.0275e-04, 2.4445e-02, 7.9859e-03, 2.3292e-04, 2.0356e-02, 1.8703e-02,\n",
            "        3.1378e-04, 6.5560e-02, 1.5749e-01, 9.5593e-02, 1.0728e-01, 1.3018e-02,\n",
            "        3.5483e-02, 1.1571e-02, 3.3617e-02, 1.3363e-02, 3.1799e-02, 3.3795e-02,\n",
            "        1.1277e-01, 2.0618e-04, 1.4773e-04, 2.7142e-04, 1.7224e-01, 1.7291e-04,\n",
            "        1.5910e-04, 4.7466e-03, 1.1477e-04, 7.6637e-02, 6.4210e-02, 3.8356e-03,\n",
            "        9.0700e-03, 1.3018e-02, 3.1949e-02, 2.9733e-04]), 'depth': tensor([ 1149.,  6478.,  4444.,  3980.,  6850.,  2841.,  5844.,  2151.,  5164.,\n",
            "         5571.,  3609.,  3607.,  4621.,  3708.,  4482.,  2663., 23807.,  3917.,\n",
            "         4050.,  2161.,  6427.,  2605.,  2266.,  2034.,  9118.,  2563.,  1504.,\n",
            "         1601.,  2837.,  1645.,  4130.,  8535., 15514.,  2105.,  3051.,  1500.,\n",
            "         3049.,  2328.,  8889.,  2762.,  5223.,  6030.,  1138.,  1702.,  4462.,\n",
            "         3980.,  7410.,  7727.,  4714.,  2563.,  1984.,  5015.,  9833.,  1977.,\n",
            "        10714.,  1554.,  3896.,  6008.,  2098.,  1203.,  2563.,  6030.,  7775.,\n",
            "         2645.])}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/292 [00:03<?, ?it/s]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in tqdm.tqdm(datamodule.train_dataloader()):\n",
        "    # pass #or do pass\n",
        "    print(i)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# .. \n",
        "# with lightning:\n",
        "# Trainer(model, datamodule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (WIP) build a set of different collators that can be used to preprocess the minibatches before feeding them to the model "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "scprint",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
