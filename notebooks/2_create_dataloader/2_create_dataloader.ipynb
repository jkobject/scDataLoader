{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scdataloader import Dataset\n",
    "from scdataloader import dataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## var definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organism = lb.Organism.filter(ontology_id=\"NCBITaxon:9606\").one()\n",
    "genedf = lb.Gene.filter(organism_id=organism.id).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Var location \n",
    "\n",
    "here we decide to add another layer of information where we provide a gene's rough location estimation in the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading gene names from biomart\n",
      "\n",
      "['ensembl_gene_id', 'hgnc_symbol', 'gene_biotype', 'entrezgene_id', 'start_position', 'chromosome_name']\n"
     ]
    }
   ],
   "source": [
    "from scdataloader.utils import getBiomartTable\n",
    "\n",
    "biomart = getBiomartTable(attributes=['start_position', 'chromosome_name']).set_index('ensembl_gene_id')\n",
    "genedf = genedf.set_index('ensembl_gene_id')\n",
    "\n",
    "genedf = genedf.loc[~genedf.index.duplicated(keep='first')]\n",
    "biomart = biomart.loc[~biomart.index.duplicated(keep='first')]\n",
    "\n",
    "genedf = genedf.join(biomart[['start_position', 'chromosome_name']], how='inner').sort_values(by=['chromosome_name', 'start_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced the size to 0.6722574020195106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = []\n",
    "i = 0\n",
    "prev_position = -100000\n",
    "prev_chromosome = None\n",
    "for _, r in genedf.iterrows():\n",
    "    if r['chromosome_name'] != prev_chromosome or r['start_position'] - prev_position > 10_000:\n",
    "        i += 1\n",
    "    c.append(i)\n",
    "    prev_position = r['start_position']\n",
    "    prev_chromosome = r['chromosome_name']\n",
    "print(f'reduced the size to {len(set(c))/len(genedf)}')\n",
    "genedf['group'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[File(uid='AnalH1SNJ2cQ7SVtsAvg', suffix='.h5ad', accessor='AnnData', description='preprocessed by scprint', version='2', size=59079604, hash='4f0no-pjg35qG--75wu5JZ', hash_type='sha1-fl', visibility=1, key_is_virtual=True, updated_at=2023-12-12 13:16:03 UTC, storage_id=1, initial_version_id=990, created_by_id=1), \n",
    "\n",
    "#File(uid='qsmZFgVcwPqVN9h23x6p', suffix='.h5ad', accessor='AnnData', description='preprocessed by scprint', version='2', size=82350434, hash='lUJl8wVAqHv1WM829YtELW', hash_type='sha1-fl', visibility=1, key_is_virtual=True, updated_at=2023-12-12 13:27:33 UTC, storage_id=1, initial_version_id=1034, created_by_id=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optional: var embeddings\n",
    "\n",
    "Many novel models like transformers work on embeddings of the variable field. This can be learnt or provided like it is done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed(genedf=genedf,\n",
    "    organism=\"homo_sapiens\",\n",
    "    cache=True,\n",
    "    fasta_path=\"/tmp/data/fasta/\",\n",
    "    embedding_size=1024,)\n",
    "embeddings.to_parquet('../../data/temp/embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.read_parquet('../../data/temp/embeddings.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loader\n",
    "\n",
    "to create the dataloader we need a lamindb dataset. Here we take the one that we created in the previous notebook, but it can be another dataset like the lamin's cellxgene dataset.\n",
    "\n",
    "example:\n",
    "```python\n",
    "dataset = ln.Dataset.using(\"laminlabs/cellxgene\").one()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OR directly load the dataset\n",
    "name=\"preprocessed dataset\"\n",
    "dataset = ln.Dataset.filter(name=name).one()\n",
    "dataset.artifacts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataloader can weight some rare samples more: \n",
    "# one need to provide the labels on which to weight the samples:\n",
    "labels_weighted_sampling = hierarchical_labels+[\n",
    "    'sex_ontology_term_id',\n",
    "    \"cell_type_ontology_term_id\",\n",
    "    #\"tissue_ontology_term_id\",\n",
    "    \"disease_ontology_term_id\",\n",
    "    #\"development_stage_ontology_term_id\",\n",
    "    \"assay_ontology_term_id\",\n",
    "    'self_reported_ethnicity_ontology_term_id',\n",
    "]\n",
    "\n",
    "# the dataloader can also output some obs field\n",
    "all_labels = labels_weighted_sampling+[\n",
    "    #'dataset_id',\n",
    "    #'cell_culture',\n",
    "    \"dpt_group\",\n",
    "    \"heat_diff\",\n",
    "    \"nnz\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ no run & transform get linked, consider passing a `run` or calling ln.track()\n",
      "won't do any check but we recommend to have your dataset coming from local storage\n",
      "❗ no run & transform get linked, consider passing a `run` or calling ln.track()\n",
      "total dataset size is 23.47712381 Gb\n",
      "---\n",
      "❗ no run & transform get linked, consider passing a `run` or calling ln.track()\n",
      "total dataset size is 23.47712381 Gb\n",
      "---\n",
      "dataset contains:\n",
      "     1582328 cells\n",
      "     70116 genes\n",
      "     8 labels\n",
      "     1 organisms\n",
      "dataset contains 113 classes to predict\n",
      "embedding size is 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#we then create a mapped dataset. This transforms a bunch of anndata from possibly various species, into a combined object that acts roughly as a single anndata dataset \n",
    "# (WIP to get all the features of an anndata object) \n",
    "mdataset = Dataset(dataset, genedf, gene_embedding=embeddings, organisms=['\"NCBITaxon:9606\"'], obs=all_labels, encode_obs=labels_weighted_sampling, map_hierarchy=hierarchical_labels, )\n",
    "mdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we make the dataloader\n",
    "dataloader = BaseDataLoader(mdataset, label_to_weight=labels_weighted_sampling, batch_size=4, num_workers=1)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64), tensor([30, 65, 78,  4]), tensor([5, 3, 3, 5]), tensor([1, 1, 1, 1]), tensor([2, 5, 3, 3]), tensor([0, 0, 1, 1]), ('13_MONDO:0100320_CL:0001062_UBERON:0000178', '2_PATO:0000461_CL:0000907_UBERON:0000178', '0_PATO:0000461_CL:0000938_UBERON:0000178', '7_MONDO:0100320_CL:0000794_UBERON:0000178'), tensor([0.0027, 0.0066, 0.0040, 0.0029], dtype=torch.float64), tensor([1206, 1953, 1005,  787])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (WIP) build a set of different collators that can be used to preprocess the minibatches before feeding them to the model "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
